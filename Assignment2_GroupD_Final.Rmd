---
title: "CSDA1040 Assignment 2: Hotel Sentiment Analysis"
#subtitle: "A Simple, Sentiment Analysis Application to Determine Positive Or Negative TripAdvisor Reviews"
author: "Amitabh Kumar, Joseph Gyamfi, (Jamie) Yeon Ju Heo, and Rob Ness"
date: "July 14, 2019"
output: html_document
#name: "CSDA1040 Advanced Methods of Data Analysis"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract
Using a TripAdvisor hotel reviews for the Hilton Hawaiian Village Waikiki Beach Resort in Hawaii, our project focuses around text mining for sentiment analysis.  To prepare the data for analysis, we used various text pre-processing techniques including stemming and stop-word elimination.  We then performed sentiment analysis on the corpus after creating a Term Document Matrix.  Finally, we designed an R Shiny application to help visualize the most frequent words in various forms including a word cloud among others.

# Introduction and Discussion
With the advent of social media, planning for a dream vacation or an all-important business trip has undergone a paradigm shift.  No longer examining glossy colour brochures or discussing with a knowledgeable travel advisor are the initial go-to solutions.

Hotel review websites such as TripAdvisor provide an online repository where past guests share their experiences, and potential guests may review these experiences to determine if the property meets their needs.

However, the volume of information available is increasing daily, and where a dozen reviews may have once been the norm and easily reviewed, tens of thousands of reviews are now accessible at the click of a button - and impractical to truly understand the overall sentiment.  The problem remains: how to effectively derive insight about a property based on thousands of text-based historical experiences?

Our project is intended to perform a quantitative sentiment analysis of the words used in these reviews by mining the most frequent key words appearing within the reviews.  Additionally, we developed an exploratory application to serve as a text mining visualization tool which helps identify the most common words within the collection of reviews.

Ultimately, we hope to present this an initial proof of concept and will serve to define the sentimentality of key features and their popularity at a given hotel property.

# Dataset Description
For our data source, we used a dataset obtained from https://github.com/susanli2016/Data-Analysis-with-R/blob/master/Hilton_Hawaiian_Village_Waikiki_Beach_Resort-Honolulu_Oahu_Hawaii__en.csv. The raw dataset consists of 13,701 reviews that were submitted between March 21, 2002 and August 2, 2018, and is spread across 2 variables defined as review date and review comment. For our analysis, 5,388 reviews, which were submitted from August 1, 2015 to August 2, 2018 were selected due to the file size for text mining.


```{r}
# Initial Setup: The following packages required for text mining are loaded.
library(dplyr)
library(readr)
library(lubridate)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(stringr)
library(tidyr)
library(scales)
library(broom)
library(purrr)
library(widyr)
library(igraph)
library(ggraph)
library(SnowballC)
library(wordcloud)
library(reshape2)
library(sparklyr)
library(qdap)
library(tm)
library(plotrix)
library(dendextend)
library(ggthemes)
library(RWeka)
library(quanteda)
theme_set(theme_minimal())
```

# The Data

```{r}
review2 <- read_csv("Hilton_Hawaiian_Village_Waikiki_Beach_Resort-Honolulu_Oahu_Hawaii__en.csv")
names(review2)
```


```{r}
review2 <- review2[complete.cases(review2), ]
review2$review_date <- as.Date(review2$review_date, format = "%d-%B-%y")
```

```{r}
dim(review2); min(review2$review_date); max(review2$review_date)
```

```{r}
review3 <- subset(review2, review_date >="2015-08-01")
dim(review3); min(review3$review_date); max(review3$review_date)
```

We selected the 5388 reviews submitted from 2015-08-01 to 2018-08-02.

```{r}
review4 <- tibble::rowid_to_column(review3, "ID")
review4 <- review4 %>%
  mutate(review_date = as.POSIXct(review_date, origin = "1970-01-01"),month = round_date(review_date, "month"))
```

## Text extraction

```{r}
# Make a vector source and a corpus
corpus_review4=Corpus(VectorSource(review4$review_body))
```

## Text Pre-processing
After obtaining the corpus, the next step will be cleaning and preprocessing of the text. We used fuctions from the tm and qdap packages. In bag of words text mining, cleaning helps aggregate terms. For example, it may make sense that the words "miner", "mining" and "mine" should be considered one term.
Common preprocessing functions include tolower(), removePunctuation(), removeWords(), etc.

```{r}
# 1. Convert to lower case: if there are 2 words "Dress" and "dress", it will be converted to a single entry "dress"
corpus_review4=tm_map(corpus_review4, tolower)
```
```{r}
# 2. Remove punctuation:
corpus_review4=tm_map(corpus_review4, removePunctuation)
```
```{r}
# 3. Remove stopwords:
corpus_review4=tm_map(corpus_review4, removeWords, stopwords("english"))
```

## Stemming a document
Another useful preprocessing step involves word stemming and stem completion. The tm package provides the stemDocument() function to get to a word's root. This function either takes in a character vector and returns a character vector, or takes in a plainTextDocument and returns a PlainTextDocument.

```{r}
# Stem document
corpus_review4=tm_map(corpus_review4, stemDocument)

# Viewing the corpus content
corpus_review4[[8]][1]
```

# Find the 20 most frequent terms: term_count

```{r}
term_count <- freq_terms(corpus_review4, 20)
# Plot 20 most frequent terms
plot(term_count)
```

The words "roon", "stay"", "hotel", "tower", "beach", etc are the most frequently used words.

# Create the DTM & TDM from the corpus

```{r}
review_dtm4 <- DocumentTermMatrix(corpus_review4)
review_tdm4 <- TermDocumentMatrix(corpus_review4)
```

```{r}
# Convert TDM to matrix
review_m <- as.matrix(review_tdm4)

# Sum rows and frequency data frame
review_term_freq <- rowSums(review_m)

# Sort term_frequency in descending order
review_term_freq <- sort(review_term_freq, decreasing = T)

# View the top 10 most common words
review_term_freq[1:10]
```

```{r}
# Plot a barchart of the 20 most common words
barplot(review_term_freq[1:20], col = "steel blue", las = 2)
```


```{r}
# Word clouds
review_word_freq <- data.frame(term = names(review_term_freq),
  num = review_term_freq)
# Create a wordcloud for the values in word_freqs
wordcloud(review_word_freq$term, review_word_freq$num,
  max.words = 50, colors = c("aquamarine","darkgoldenrod","tomato"))
```

# Bigrams

We often want to understand the relationship between words in a review. What sequences of words are common across review text? Given a sequence of words, what word is most likely to follow? What words have the strongest relationship with each other? Therefore, many interesting text analysis are based on the relationships. When we exam pairs of two consecutive words, it is often called bigrams.

So, what are the most common bigrams Hilton Hawaiian Village's TripAdvisor reviews?

```{r}
review_bigrams <- review4 %>%
  unnest_tokens(bigram, review_body, token = "ngrams", n = 2)

bigrams_separated <- review_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%
  count(bigram, sort = TRUE)
```

The most common bigrams is "rainbow tower", followed by "hawaiian village" and "hilton hawiian".

# Trigrams
Bigrams sometimes are not enough, let's see what are the most common trigrams in Hilton Hawaiian Village's TripAdvisor reviews?

```{r}
review_trigrams <- review4 %>%
  unnest_tokens(trigram, review_body, token = "ngrams", n = 3)

trigrams_separated <- review_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

trigram_counts <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

trigrams_united %>%
  count(trigram, sort = TRUE)
```

The most common trigram is "hilton hawaiian village", followed by "friday night fireworks" and "diamond head tower".

# Sentiment Analysis

```{r}
reviews4_1 <- review4 %>% 
  filter(!is.na(review_body)) %>% 
  select(ID, review_body) %>% 
  group_by(row_number()) %>% 
  ungroup()
tidy_reviews <- reviews4_1 %>%
  unnest_tokens(word, review_body)
tidy_reviews <- tidy_reviews %>%
  anti_join(stop_words)

bing_word_counts <- tidy_reviews %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  labs(y = "Contribution to sentiment", x = NULL) +
  coord_flip() + 
  ggtitle('Words that contribute to positive and negative sentiment in the reviews')
```

The most common positive words in the reviews are "nice", "clean" and "beautiful".
The most negative words in the reviews are "expensive" and "crowded".

## Use "affin" to produce numeric score sentiment column

```{r}
contributions <- tidy_reviews %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  rename(score = value)%>%
  summarize(occurences = n(),
           contribution = sum(score))
```

```{r}
contributions %>%
  top_n(25, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  ggtitle('Words with the greatest contributions to positive/negative 
          sentiment in reviews') +
  geom_col(show.legend = FALSE) +
  coord_flip()

```


## Using bigrams to provide context in sentiment analysis

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

```{r}
AFINN <- get_sentiments("afinn")%>%
  rename(score = value)

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words
```


```{r}
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  ggtitle('The 20 words preceded by "not" that had the greatest contribution to 
          sentiment scores, positive or negative direction') +
  coord_flip()
```

```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negated_words %>%
  mutate(contribution = n * score,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  top_n(12, abs(contribution)) %>%
  ggplot(aes(word2, contribution, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment score * # of occurrences") +
  ggtitle('The most common positive or negative words to follow negations 
          such as "no", "not", "never" and "without"') +
  coord_flip()
```


```{r}
sentiment_messages <- tidy_reviews %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  rename(score = value)%>%
  group_by(ID) %>%
  summarize(sentiment = mean(score),
            words = n()) %>%
  ungroup() %>%
  filter(words >= 5)

sentiment_messages %>%
  arrange(desc(sentiment))

review4[which(review4$ID==2363), ]$review_body[1]
```


```{r}
sentiment_messages %>%
  arrange(sentiment)

review4[ which(review4$ID==3748), ]$review_body[1]
```

# Conclusion
Text mining thousands of single-property hotel reviews from a website such as TripAdvisor requires the ability to extract, cleanse, and understand the most frequently used words, the sentiment associated with the word - either positive or negative - and ultimately provide a quantitative score to help assess overall sentimentality.

This project helped us better understand the intricacies of text mining, and the abstract nature of determining the overall sentiment in a collection of words.  For instance, through bigram analysis we were able to extract words that may be perceived as positive but are negative because they were preceded by a negative connotation.

Naturally, sentiment analysis is deeply nuanced, and more analysis would be required before we can consider a formal proof of concept.  Evolution of our work would provide the ability to select any published hotel, and roll-up to a chain-level sentimentality to help classify hotel ratings more effectively - ultimately saving valuable time for vacation planners, business travellers, and travel advisors alike. 



